{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column, Series and DummyColumn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is to study the behaviour when moving different objects across GPUs via UCX. We move three different kinds of objects:\n",
    "\n",
    "\n",
    "1. cuDF `Column` objects (specifically `NumericalColumn` objects)\n",
    "2. cuDF `Series` objects\n",
    "3. `DummyColumn` objects (see `dummy.py`)\n",
    "\n",
    "    `DummyColumn` helps conveniently create objects to test communicating different kinds of objects\n",
    "    \n",
    "    #### Examples:\n",
    "    \n",
    "    ```python\n",
    "    a = DummyColumn(size=10000, kind='cupy')  # serializing a yields a CuPy array of size 10000\n",
    "    b = DummyColumn(size=100, kind='numba')   # serializing b yields a Numba DeviceArray\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting dask scheduler and workers via CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the commands below to start a dask scheduler and workers respectively. Importantly, the `CUDA_VISIBLE_DEVICES` on the workers should be set in a cyclic fashion, e.g., `2,3` and `3,2`, NOT `2,3` and `2,3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ SCHEDULER=1 UCX_RNDV_SCHEME=put_zcopy UCX_MEMTYPE_CACHE=n UCX_TLS=rc,cuda_copy,cuda_ipc CUDA_VISIBLE_DEVICES=2,3 dask-scheduler --interface ib0 --protocol ucx\n",
    "\n",
    "$ UCX_RNDV_SCHEME=put_zcopy UCX_MEMTYPE_CACHE=n UCX_TLS=rc,cuda_copy,cuda_ipc CUDA_VISIBLE_DEVICES=3,2 dask-worker ucx://10.33.225.165:8786 --nthreads=1 --memory-limit 32gb --no-nanny --protocol=ucx --name=worker_0\n",
    "\n",
    "$ UCX_RNDV_SCHEME=put_zcopy UCX_MEMTYPE_CACHE=n UCX_TLS=rc,cuda_copy,cuda_ipc CUDA_VISIBLE_DEVICES=2,3 dask-worker ucx://10.33.225.165:8786 --nthreads=1 --memory-limit 32gb --no-nanny --protocol=ucx --name=ashwint_worker_1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking nvlink utilization\n",
    "\n",
    "Use this command to monitor nvlink utilization when communicating device objects:\n",
    "\n",
    "Here `-i 2` refers to the ID of the GPU to check and `-g 1` refers to a \"counter ID\" - we've found only `1` to be helpful.\n",
    "\n",
    "```\n",
    "$ nvidia-smi nvlink -r 1  # reset counters\n",
    "$ watch -n1 nvidia-smi nvlink -g 1 -i 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some initial set up and checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_env = {\n",
    "    \"NOTEBOOK\": \"1\",\n",
    "    \"UCX_RNDV_SCHEME\": \"put_zcopy\",\n",
    "    \"UCX_MEMTYPE_CACHE\": \"n\",\n",
    "    \"UCX_TLS\": \"rc,cuda_copy,cuda_ipc\",\n",
    "    \"CUDA_VISIBLE_DEVICES\": \"2,3\",\n",
    "}\n",
    "os.environ.update(base_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Dask: {dask.__file__}')\n",
    "print(f'Distributed: {distributed.__file__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to the Dask cluster started on the CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, wait\n",
    "# from dask_cuda import DGX\n",
    "\n",
    "#cluster = DGX(CUDA_VISIBLE_DEVICES=[2,3], \n",
    "#              dashboard_address='10.33.227.165:8789')\n",
    "#client = Client(cluster)\n",
    "client = Client('ucx://10.33.225.165:8786')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting CUDA Context on the workers - important!\n",
    "\n",
    "This needs to be the first thing that happens on all workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can add it to your global config with the following yaml\n",
    "#     distributed:\n",
    "#       worker:\n",
    "#         preload:\n",
    "#           - dask_cuda.initialize_context\n",
    "def set_nb_context():\n",
    "    import numba.cuda\n",
    "    try:\n",
    "        numba.cuda.current_context()\n",
    "    except Exception:\n",
    "        print(\"FAILED EXCEPTION!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.run(set_nb_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing worker environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env():\n",
    "    import os\n",
    "    return os.environ[\"CUDA_VISIBLE_DEVICES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.run(get_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding `pwd` to `sys.path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_path(path):\n",
    "    import sys\n",
    "    sys.path.append(path)\n",
    "    return sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.run(set_path, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting worker IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_1, worker_2 = client.scheduler_info()['workers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import numpy as np\n",
    "import cupy\n",
    "from dummy import DummyColumn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving DummyColumn objects serializing to cupy - works\n",
    "\n",
    "This works and also registers on the NVLINK counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import wait\n",
    "\n",
    "left = client.map(lambda x: DummyColumn(10000, \"cupy\"), range(100), workers=[worker_1])\n",
    "right = client.map(lambda x: DummyColumn(10000, \"cupy\"), range(100), workers=[worker_2])\n",
    "results = client.map(lambda x,y: (x,y), left, right, priority=10)\n",
    "_ = wait(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving DummyColumn objects serializing to numba device arrays - works\n",
    "\n",
    "This works and also registers on the NVLINK counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import wait\n",
    "\n",
    "left = client.map(lambda x: DummyColumn(10000, \"numba\"), range(100), workers=[worker_1])\n",
    "right = client.map(lambda x: DummyColumn(10000, \"numba\"), range(100), workers=[worker_2])\n",
    "results = client.map(lambda x,y: (x,y), left, right, priority=10)\n",
    "_ = wait(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving DummyColumn objects serializing to RMM backed device arrays - works\n",
    "\n",
    "This works and also registers on the NVLINK counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import wait\n",
    "\n",
    "left = client.map(lambda x: DummyColumn(10000, \"rmm\"), range(100), workers=[worker_1])\n",
    "right = client.map(lambda x: DummyColumn(10000, \"rmm\"), range(100), workers=[worker_2])\n",
    "results = client.map(lambda x,y: (x,y), left, right, priority=10)\n",
    "_ = wait(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving cuDF Column objects - works\n",
    "\n",
    "This works and also registers on the NVLINK counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import wait\n",
    "\n",
    "left = client.map(lambda x: cudf.Series(np.arange(10000))._column, range(100), workers=[worker_1])\n",
    "right = client.map(lambda x: cudf.Series(np.arange(10000))._column, range(100), workers=[worker_2])\n",
    "results = client.map(lambda x,y: (x,y), left, right, priority=10)\n",
    "_ = wait(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving cuDF Series objects - does NOT work\n",
    "\n",
    "This does not work, although it *might* register on the NVLINK counter until it hangs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import wait\n",
    "\n",
    "left = client.map(lambda x: cudf.Series(np.arange(10000)), range(100), workers=[worker_1])\n",
    "right = client.map(lambda x: cudf.Series(np.arange(10000)), range(100), workers=[worker_2])\n",
    "results = client.map(lambda x,y: (x,y), left, right, priority=10)\n",
    "_ = wait(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
